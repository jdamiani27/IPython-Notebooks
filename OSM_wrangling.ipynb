{
 "metadata": {
  "name": "",
  "signature": "sha256:a9c4d617b7d5601b2d3876d1f648d8e938f02ab370967776c5245e62bd206a0f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Wrangling OpenStreetMap Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OpenStreetMap is a community built map of the world, similar in nature to how content on Wiki sites is generated and maintained.  \n",
      "\n",
      "[http://www.openstreetmap.org](http://www.openstreetmap.org)\n",
      "\n",
      "Users can map things such as polylines of roads, draw polygons of buildings or areas of interest, or insert nodes for landmarks.  These map elements can be further tagged with details such as street addresses or amenity type.  Map data is stored in an XML format.  More details about the OSM XML can be found here:\n",
      "\n",
      "[http://wiki.openstreetmap.org/wiki/OSM_XML](http://wiki.openstreetmap.org/wiki/OSM_XML)\n",
      "\n",
      "Some highlights of the OSM XML format relevent to this project are:\n",
      "\n",
      "* OSM XML is list of instances of data primatives (nodes, ways, and relations) found within a given bounds\n",
      "* nodes represent dimensionless points on the map\n",
      "* ways contain node references to form either a polyline or polygon on the map\n",
      "* nodes and ways both contain children tag elements that represent key value pairs of descriptive information about a given node or way\n",
      "\n",
      "As with any user generated content, there is likely going to be dirty data.  In this project I'll attempt to do some auditing, cleaning, and data summarizing tasks with Python and MongoDB."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Chosen map area"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this project, I chose to analyze data from Volusia and Flagler Counties, Florida. I grew up in this relatively rural area.  I figure that my familiarity with the area and my hunch that this rural area has yet to be be thouroughly audited on the OpenStreetMap platform make it a good candidate for analysis.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('<iframe width=\"425\" height=\"350\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" src=\"http://www.openstreetmap.org/export/embed.html?bbox=-81.793212890625%2C28.75441649498853%2C-80.85113525390625%2C29.756032197482973&amp;layer=mapnik\"></iframe><br/><small><a href=\"http://www.openstreetmap.org/#map=10/29.2565/-81.3222\" target=\"_blank\">View Larger Map</a></small>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe width=\"425\" height=\"350\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" src=\"http://www.openstreetmap.org/export/embed.html?bbox=-81.793212890625%2C28.75441649498853%2C-80.85113525390625%2C29.756032197482973&amp;layer=mapnik\"></iframe><br/><small><a href=\"http://www.openstreetmap.org/#map=10/29.2565/-81.3222\" target=\"_blank\">View Larger Map</a></small>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x106796fd0>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I used the Overpass API to download the OpenStreetMap XML for the corresponding bounding box:\n",
      "\n",
      "[http://overpass-api.de/api/map?bbox=-81.5600,28.8400,-80.7400,29.6713](http://overpass-api.de/api/map?bbox=-81.5600,28.8400,-80.7400,29.6713)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "\n",
      "url = 'http://overpass-api.de/api/map?bbox=-81.5600,28.8400,-80.7400,29.6713'\n",
      "filename = 'volusia_flagler.osm'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While easy to use, the Python requests library does not have save functionality.  The below code is modified from this stackoverflow post:\n",
      "\n",
      "[http://stackoverflow.com/a/16696317](http://stackoverflow.com/a/16696317)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def download_file(url, local_filename):\n",
      "    # stream = True allows downloading of large files; prevents loading entire file into memory\n",
      "    r = requests.get(url, stream = True)\n",
      "    with open(local_filename, 'wb') as f:\n",
      "        for chunk in r.iter_content(chunk_size=1024): \n",
      "            if chunk: # filter out keep-alive new chunks\n",
      "                f.write(chunk)\n",
      "                f.flush()\n",
      "                \n",
      "download_file(url, filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Auditing the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the OSM XML file downloaded, I will parse through it with ElementTree and find the number of each type of element.  In this project, I will use iterative parsing as the XML download can be too large to work with in memory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import pprint\n",
      "\n",
      "tags = {}\n",
      "\n",
      "for event, elem in ET.iterparse(filename):\n",
      "    if elem.tag in tags:\n",
      "        tags[elem.tag] += 1\n",
      "    else:\n",
      "        tags[elem.tag] = 1\n",
      "        \n",
      "pprint.pprint(tags)        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'bounds': 1,\n",
        " 'member': 26677,\n",
        " 'meta': 1,\n",
        " 'nd': 362286,\n",
        " 'node': 308585,\n",
        " 'note': 1,\n",
        " 'osm': 1,\n",
        " 'relation': 331,\n",
        " 'tag': 214955,\n",
        " 'way': 28359}\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here I have built three regular expressions: ```lower```, ```lower_colon```, and ```problemchars```. \n",
      "\n",
      "* ```lower```: matches strings containing lower case characters\n",
      "* ```lower_colon```: matches strings containing lower case characters and a single colon within the string\n",
      "* ```problemchars```: matches characters that cannot be used within keys in MongoDB\n",
      "\n",
      "Here is a sample of OSM XML:\n",
      "\n",
      "```xml\n",
      "<node id=\"358710668\" lat=\"29.2008100\" lon=\"-81.0536700\" version=\"2\" timestamp=\"2010-05-30T02:03:18Z\" changeset=\"4847960\" uid=\"294662\" user=\"Kenny Carte\">\n",
      "    <tag k=\"addr:housenumber\" v=\"303\"/>\n",
      "    <tag k=\"addr:street\" v=\"North Clyde Morris BLVD\"/>\n",
      "</node>\n",
      "```\n",
      "\n",
      "Within the ```node``` element there are two ```tag``` children.  The key for both of these children begins with ```addr:```.  Later in this notebook I will use the ```lower_colon``` regex to help find these keys so I can build a single ```address``` document within a larger json document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "lower = re.compile(r'^([a-z]|_)*$')\n",
      "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
      "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
      "\n",
      "def key_type(element, keys):\n",
      "    if element.tag == \"tag\":\n",
      "        \n",
      "        if problemchars.search(element.attrib['k']):\n",
      "            keys['problemchars'] += 1\n",
      "        elif lower.search(element.attrib['k']):\n",
      "            keys['lower'] += 1\n",
      "        elif lower_colon.search(element.attrib['k']):\n",
      "            keys['lower_colon'] += 1 \n",
      "        else:\n",
      "            keys['other'] += 1\n",
      "        \n",
      "    return keys\n",
      "\n",
      "def process_map(filename):\n",
      "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
      "    \n",
      "    for _, element in ET.iterparse(filename):\n",
      "        keys = key_type(element, keys)\n",
      "\n",
      "    return keys\n",
      "\n",
      "keys = process_map(filename)\n",
      "pprint.pprint(keys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'lower': 79407, 'lower_colon': 128362, 'other': 7185, 'problemchars': 1}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I will redefine ```process_map``` to build a set of unique userid's found within the XML.  I will then output the length of this set, representing the number of unique users making edits in the chosen map area."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_map(filename):\n",
      "    users = set()\n",
      "    for _, element in ET.iterparse(filename):\n",
      "        if \"uid\" in element.attrib and element.tag in ('node', 'way'):\n",
      "            users.add(element.attrib[\"uid\"])\n",
      "\n",
      "    return users\n",
      "\n",
      "users = process_map(filename)\n",
      "len(users)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "260"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cleaning Street Names"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The majority of this project will be devoted to auditing and cleaning street names seen within the OSM XML.  Street types used by users in the process of mapping are quite often abbreviated.  I will attempt to find these abbreviations and replace them with their full text form.  The plan of action is as follows:\n",
      "\n",
      "* Build a regex to match the last token in a string (with an optional '.') as this is typically where you would find the street type in an address\n",
      "* Build a list of expected street types that do not need to be cleaned\n",
      "* Parse through the XML looking for ```tag``` elements with ```k=\"addr:street\"``` attributes\n",
      "* Perform a search using the regex on the value of the ```v``` attribute of these elements (the street name string)\n",
      "* Build a dictionary with keys that are matches to the regex (street types) and a set of street names where the particular key was found as the value.  This will allow us to determine what needs to be cleaned.\n",
      "* Build a second dictionary that contains a map from an offending street type to a clean street type\n",
      "* Build a second regex that will match these offending street types anywhere in a string\n",
      "* Build a function that will return a clean string using the mapping dictionary and this second regex\n",
      "\n",
      "The first step is to build a regex to match the last token in a string optionally ending with a period.  I will also build a list of street types I expect to see in a clean street name."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
      "\n",
      "expected_street_types = [\"Avenue\", \"Boulevard\", \"Commons\", \"Court\", \"Drive\", \"Lane\", \"Parkway\", \n",
      "                         \"Place\", \"Road\", \"Square\", \"Street\", \"Trail\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The ```audit_string``` function will take in the dictionary of street types we are building, a string to audit, a regex to match against that string, and the list of expected street types.\n",
      "\n",
      "The function will search the string for the regex.  If there is a match and the match is not in our list of expected street types, add the match as a key to the dictionary and add the string to the set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def audit_string(match_set_dict, string_to_audit, regex, expected_matches):\n",
      "    \n",
      "    m = regex.search(string_to_audit)\n",
      "    \n",
      "    if m:\n",
      "        match_string = m.group()\n",
      "        \n",
      "        if match_string not in expected_matches:\n",
      "            match_set_dict[match_string].add(string_to_audit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I will define an ```audit``` function to do the parsing and auditing of the street names.\n",
      "\n",
      "I have defined this function so that it not only audits ```tag``` elements where ```k=\"addr:street\"```, but whichever ```tag``` elements match the ```tag_filter``` function.  The ```audit``` function also takes in a regex and the list of expected matches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def audit(osmfile, tag_filter, regex, expected_matches = []):\n",
      "    \n",
      "    osm_file = open(osmfile, \"r\")\n",
      "    match_sets = defaultdict(set)\n",
      "    \n",
      "    # iteratively parse the mapping xml\n",
      "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
      "        # node and way tags are of special interest\n",
      "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
      "            # iterate the \"tag\" tags within a node or way\n",
      "            for tag in elem.iter(\"tag\"): \n",
      "                if tag_filter(tag):\n",
      "                    audit_string(match_sets, tag.attrib['v'], regex, expected_matches)\n",
      "    \n",
      "    return match_sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function ```is_street_name``` determines if an element contains an attribute ```k=\"addr:street\"```.  I will use ```is_street_name``` as the ```tag_filter``` when I call the ```audit``` function to audit street names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_street_name(elem):\n",
      "    return (elem.attrib['k'] == \"addr:street\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets pretty print the output of ```audit```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "street_types = audit(filename, tag_filter = is_street_name, regex = street_type_re, \n",
      "                     expected_matches = expected_street_types)\n",
      "\n",
      "pprint.pprint(dict(street_types))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'1': set(['US 1']),\n",
        " '100': set(['W. Highway 100']),\n",
        " 'Ave': set(['Central Ave',\n",
        "             'E Arizona Ave',\n",
        "             'E Euclid Ave',\n",
        "             'E MIchigan Ave',\n",
        "             'E Michigan Ave',\n",
        "             'E Minnesota Ave',\n",
        "             'E Pennsylania Ave',\n",
        "             'E Pennsylvania Ave',\n",
        "             'E Stetson Ave',\n",
        "             'E University Ave',\n",
        "             'N Amelia Ave',\n",
        "             'Rhode Island Ave',\n",
        "             'W Minnesota Ave',\n",
        "             'W Pennsylvania Ave']),\n",
        " 'BLVD': set(['North Clyde Morris BLVD']),\n",
        " 'Blvd': set(['Commerce Blvd',\n",
        "              'Harley Strickland Blvd',\n",
        "              'Howland Blvd',\n",
        "              'Mahogany Blvd',\n",
        "              'N Woodland Blvd',\n",
        "              'S Clyde Morris Blvd',\n",
        "              'S Woodland Blvd',\n",
        "              'Seasame Blvd',\n",
        "              'Town Center Blvd',\n",
        "              'W International Speedway Blvd',\n",
        "              'W Intl Speedway Blvd',\n",
        "              'West Granada Blvd']),\n",
        " 'Blvd.': set(['West International Speedway Blvd.']),\n",
        " 'Cir': set(['Fraternity Cir']),\n",
        " 'Circle': set(['Huntington Village Circle']),\n",
        " 'Dr': set(['Cypress Edge Dr',\n",
        "            'E Bert Fish Dr',\n",
        "            'Flagler Plaza Dr',\n",
        "            'N Bert FIsh Dr',\n",
        "            'N Bert Fish Dr',\n",
        "            'Rymfire Dr']),\n",
        " 'East': set(['Palm Coast Pkwy East']),\n",
        " 'Ln': set(['Bainbridge Ln']),\n",
        " 'N': set(['Garden St N', 'Old Kings Rd N']),\n",
        " 'Pkwy': set(['City Center Pkwy', 'Palm Coast Pkwy']),\n",
        " 'Pky': set(['Pine Lakes Pky']),\n",
        " 'Rd': set(['W Highbanks Rd']),\n",
        " 'Rd.': set(['North Nova Rd.']),\n",
        " 'Run': set(['Wolf Pack Run']),\n",
        " 'South': set(['Ibis Court South']),\n",
        " 'Speedway': set([\"W. Int'l Speedway\"]),\n",
        " 'St': set(['10th St']),\n",
        " 'St.': set(['Third St.']),\n",
        " 'Way': set(['Kings Way']),\n",
        " 'West': set(['Palm Coast Pkwy West'])}\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I have a list of some abbreviated street types (as well as clean street types I did not expect, cardinal directions, and highway numbers).  This is by no means a comprehensive list of all of the abbreviated street types used within the XML as all of these matches occur only as the last token at the end of a street name, but it is a very good first swipe at the problem.\n",
      "\n",
      "To replace these abbreviated street types, I will define an ```update``` function that takes a string to update, a mapping dictionary, and a regex to search."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update(string_to_update, mapping, regex):\n",
      "\n",
      "    m = regex.search(string_to_update)\n",
      "   \n",
      "    if m:\n",
      "        match = m.group()\n",
      "        \n",
      "        if match in mapping:\n",
      "            string_to_update = re.sub(regex, mapping[match], string_to_update)\n",
      "            \n",
      "    return string_to_update"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the results of the audit, I will build a dictionary to map abbreviations to their full, clean representations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_street_types = \\\n",
      "    {\n",
      "        \"Ave\" : \"Avenue\",\n",
      "        \"BLVD\" : \"Boulevard\",\n",
      "        \"Blvd\" : \"Boulevard\",\n",
      "        \"Blvd.\" : \"Boulevard\",\n",
      "        \"Cir\" : \"Circle\",\n",
      "        \"Dr\" : \"Drive\",\n",
      "        \"Ln\" : \"Lane\",\n",
      "        \"Pkwy\" : \"Parkway\",\n",
      "        \"Rd\" : \"Road\",\n",
      "        \"Rd.\" : \"Road\",\n",
      "        \"St\" : \"Street\",\n",
      "        \"St.\" : \"Street\"\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I now want to replace the keys of the map anywhere in the string. I'll build a new regex to do so."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Take the keys from the map and create a string joined by a pipe\n",
      "# Replace '.' with the empty string, as the regex will handle the optional periods \n",
      "bad_streets = \"|\".join(map_street_types.keys()).replace('.', '')\n",
      "\n",
      "# The pipe will cause the regex to search for any of the keys, lazily matching the first it finds\n",
      "street_type_updater_re = re.compile(r'\\b(' + bad_streets + r')\\b\\.?', re.IGNORECASE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how this works, I will traverse the ```street_types``` dictionary from above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for street_type, ways in street_types.iteritems():\n",
      "    if street_type in map_street_types:\n",
      "        for name in ways:\n",
      "            better_name = update(name, map_street_types, street_type_updater_re)\n",
      "            print name, \"=>\", better_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Bainbridge Ln => Bainbridge Lane\n",
        "Third St. => Third Street\n",
        "W Highbanks Rd => W Highbanks Road\n",
        "North Clyde Morris BLVD => North Clyde Morris Boulevard\n",
        "North Nova Rd. => North Nova Road\n",
        "Rymfire Dr => Rymfire Drive\n",
        "N Bert Fish Dr => N Bert Fish Drive\n",
        "N Bert FIsh Dr => N Bert FIsh Drive\n",
        "Cypress Edge Dr => Cypress Edge Drive\n",
        "Flagler Plaza Dr => Flagler Plaza Drive\n",
        "E Bert Fish Dr => E Bert Fish Drive\n",
        "Palm Coast Pkwy => Palm Coast Parkway\n",
        "City Center Pkwy => City Center Parkway\n",
        "Fraternity Cir => Fraternity Circle\n",
        "10th St => 10th Street\n",
        "West International Speedway Blvd. => West International Speedway Boulevard\n",
        "W International Speedway Blvd => W International Speedway Boulevard\n",
        "Seasame Blvd => Seasame Boulevard\n",
        "N Woodland Blvd => N Woodland Boulevard\n",
        "S Woodland Blvd => S Woodland Boulevard\n",
        "Harley Strickland Blvd => Harley Strickland Boulevard\n",
        "Howland Blvd => Howland Boulevard\n",
        "Commerce Blvd => Commerce Boulevard\n",
        "West Granada Blvd => West Granada Boulevard\n",
        "Town Center Blvd => Town Center Boulevard\n",
        "Mahogany Blvd => Mahogany Boulevard\n",
        "W Intl Speedway Blvd => W Intl Speedway Boulevard\n",
        "S Clyde Morris Blvd => S Clyde Morris Boulevard\n",
        "E Minnesota Ave => E Minnesota Avenue\n",
        "E Euclid Ave => E Euclid Avenue\n",
        "E Arizona Ave => E Arizona Avenue\n",
        "Central Ave => Central Avenue\n",
        "E MIchigan Ave => E MIchigan Avenue\n",
        "E Stetson Ave => E Stetson Avenue\n",
        "W Pennsylvania Ave => W Pennsylvania Avenue\n",
        "E Michigan Ave => E Michigan Avenue\n",
        "W Minnesota Ave => W Minnesota Avenue\n",
        "N Amelia Ave => N Amelia Avenue\n",
        "E University Ave => E University Avenue\n",
        "E Pennsylvania Ave => E Pennsylvania Avenue\n",
        "Rhode Island Ave => Rhode Island Avenue\n",
        "E Pennsylania Ave => E Pennsylania Avenue\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like the abbreviated street types updated as expected.\n",
      "\n",
      "Upon closer inspection, I see another problem: cardinal directions.  North, South, East, and West appear to be universally abbreviated.  Lets apply similar techniques to replace these abbreviated cardinal directions.\n",
      "\n",
      "First, I will create a new regex matching the set of characters NSEW at the beginning of a string, followed by an optional period"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cardinal_dir_re = re.compile(r'^[NSEW]\\b\\.?', re.IGNORECASE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To audit, I can use the same function with this new regex"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cardinal_directions = audit(filename, is_street_name, cardinal_dir_re)\n",
      "\n",
      "pprint.pprint(dict(cardinal_directions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'E': set(['E Arizona Ave',\n",
        "           'E Bert Fish Dr',\n",
        "           'E Euclid Ave',\n",
        "           'E MIchigan Ave',\n",
        "           'E Michigan Ave',\n",
        "           'E Minnesota Ave',\n",
        "           'E Pennsylania Ave',\n",
        "           'E Pennsylvania Ave',\n",
        "           'E Stetson Ave',\n",
        "           'E University Ave']),\n",
        " 'N': set(['N Amelia Ave',\n",
        "           'N Bert FIsh Dr',\n",
        "           'N Bert Fish Dr',\n",
        "           'N Woodland Blvd']),\n",
        " 'S': set(['S Clyde Morris Blvd', 'S Woodland Blvd']),\n",
        " 'W': set(['W Highbanks Rd',\n",
        "           'W International Speedway Blvd',\n",
        "           'W Intl Speedway Blvd',\n",
        "           'W Minnesota Ave',\n",
        "           'W Pennsylvania Ave']),\n",
        " 'W.': set(['W. Highway 100', \"W. Int'l Speedway\"])}\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like we found E, N, S, W, and W. at beginning of the street names.  Informative, but I can just create an exhaustive mapping for this issue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_cardinal_directions = \\\n",
      "    {\n",
      "        \"E\" : \"East\",\n",
      "        \"E.\" : \"East\",\n",
      "        \"N\" : \"North\",\n",
      "        \"N.\" : \"North\",\n",
      "        \"S\" : \"South\",\n",
      "        \"S.\" : \"South\",\n",
      "        \"W\" : \"West\",\n",
      "        \"W.\" : \"West\"\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I now want to replace the keys anywhere in the string. I'll build a new regex to do so"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bad_directions = \"|\".join(map_cardinal_directions.keys()).replace('.', '')\n",
      "cardinal_dir_updater_re = re.compile(r'\\b(' + bad_directions + r')\\b\\.?', re.IGNORECASE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, I will traverse the ```cardinal_directions``` dictionary and apply the updates for both street type and cardinal direction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for cardinal_direction, ways in cardinal_directions.iteritems():\n",
      "    if cardinal_direction in map_cardinal_directions:\n",
      "        for name in ways:\n",
      "            better_name = update(name, map_street_types, street_type_updater_re)\n",
      "            best_name = update(better_name, map_cardinal_directions, cardinal_dir_updater_re)\n",
      "            print name, \"=>\", better_name, \"=>\", best_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "W. Int'l Speedway => W. Int'l Speedway => West Int'l Speedway\n",
        "W. Highway 100 => W. Highway 100 => West Highway 100\n",
        "S Woodland Blvd => S Woodland Boulevard => South Woodland Boulevard\n",
        "S Clyde Morris Blvd => S Clyde Morris Boulevard => South Clyde Morris Boulevard\n",
        "E Minnesota Ave => E Minnesota Avenue => East Minnesota Avenue\n",
        "E Pennsylania Ave => E Pennsylania Avenue => East Pennsylania Avenue\n",
        "E Arizona Ave => E Arizona Avenue => East Arizona Avenue\n",
        "E MIchigan Ave => E MIchigan Avenue => East MIchigan Avenue\n",
        "E Stetson Ave => E Stetson Avenue => East Stetson Avenue\n",
        "E Michigan Ave => E Michigan Avenue => East Michigan Avenue\n",
        "E University Ave => E University Avenue => East University Avenue\n",
        "E Pennsylvania Ave => E Pennsylvania Avenue => East Pennsylvania Avenue\n",
        "E Bert Fish Dr => E Bert Fish Drive => East Bert Fish Drive\n",
        "E Euclid Ave => E Euclid Avenue => East Euclid Avenue\n",
        "W International Speedway Blvd => W International Speedway Boulevard => West International Speedway Boulevard\n",
        "W Pennsylvania Ave => W Pennsylvania Avenue => West Pennsylvania Avenue\n",
        "W Highbanks Rd => W Highbanks Road => West Highbanks Road\n",
        "W Minnesota Ave => W Minnesota Avenue => West Minnesota Avenue\n",
        "W Intl Speedway Blvd => W Intl Speedway Boulevard => West Intl Speedway Boulevard\n",
        "N Woodland Blvd => N Woodland Boulevard => North Woodland Boulevard\n",
        "N Amelia Ave => N Amelia Avenue => North Amelia Avenue\n",
        "N Bert Fish Dr => N Bert Fish Drive => North Bert Fish Drive\n",
        "N Bert FIsh Dr => N Bert FIsh Drive => North Bert FIsh Drive\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparing for MongoDB"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To load the XML data into MongoDB, I will have to transform the data into json documents structured like this:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"id\": \"2406124091\",\n",
      "    \"type: \"node\",\n",
      "    \"visible\":\"true\",\n",
      "    \"created\": {\n",
      "                  \"version\":\"2\",\n",
      "                  \"changeset\":\"17206049\",\n",
      "                  \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
      "                  \"user\":\"linuxUser16\",\n",
      "                  \"uid\":\"1219059\"\n",
      "               },\n",
      "    \"pos\": [41.9757030, -87.6921867],\n",
      "    \"address\": {\n",
      "                  \"housenumber\": \"5157\",\n",
      "                  \"postcode\": \"60625\",\n",
      "                  \"street\": \"North Lincoln Ave\"\n",
      "               },\n",
      "    \"amenity\": \"restaurant\",\n",
      "    \"cuisine\": \"mexican\",\n",
      "    \"name\": \"La Cabana De Don Luis\",\n",
      "    \"phone\": \"1 (773)-271-5176\"\n",
      "}\n",
      "```\n",
      "\n",
      "The transform will follow these rules:\n",
      "\n",
      "* Process only 2 types of top level tags: ```node``` and ```way```\n",
      "* All attributes of ```node``` and ```way``` should be turned into regular key/value pairs, except:\n",
      "    * Attributes in the CREATED array should be added under a key ```created```\n",
      "    * Attributes for latitude and longitude should be added to a ```pos``` array, for use in geospacial indexing. Make sure the values inside ```pos``` array are floats and not strings. \n",
      "* If second level ```tag``` \"k\" value contains problematic characters, it should be ignored\n",
      "* If second level ```tag``` \"k\" value starts with \"addr:\", it should be added to a dictionary ```address```\n",
      "* If second level ```tag``` \"k\" value does not start with \"addr:\", but contains \":\", you can process it same as any other tag.\n",
      "* If there is a second \":\" that separates the type/direction of a street, the tag should be ignored, for example:\n",
      "\n",
      "```xml\n",
      "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
      "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
      "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
      "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
      "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
      "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
      "```\n",
      "should be turned into:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"address\": {\n",
      "                   \"housenumber\": 5158,\n",
      "                   \"street\": \"North Lincoln Avenue\"\n",
      "               },\n",
      "    \"amenity\": \"pharmacy\"\n",
      "}\n",
      "```\n",
      "\n",
      "* For \"way\" specifically:\n",
      "\n",
      "```xml\n",
      "<nd ref=\"305896090\"/>\n",
      "<nd ref=\"1719825889\"/>\n",
      "```\n",
      "should be turned into:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"node_refs\": [\"305896090\", \"1719825889\"]\n",
      "}\n",
      "```\n",
      "\n",
      "To do this transformation, I will define a function ```shape_element``` that processes an element.  Within this function I will use the ```update``` function with the regexes and mapping dictionaries defined above to clean street addresses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shape_element(element):\n",
      "    node = {}\n",
      "    CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
      "    \n",
      "    if element.tag == \"node\" or element.tag == \"way\" :\n",
      "\n",
      "      node['type'] = element.tag\n",
      "\n",
      "      # Parse attributes\n",
      "      for a in element.attrib:\n",
      "            \n",
      "        # parse details of data creation\n",
      "        if a in CREATED:\n",
      "          if 'created' not in node:\n",
      "            node['created'] = {}\n",
      "          node['created'][a] = element.attrib[a]\n",
      "        \n",
      "        # parse coordinates\n",
      "        elif a in ['lat', 'lon']:\n",
      "          if 'pos' not in node:\n",
      "            node['pos'] = [None, None]\n",
      "\n",
      "          if a == 'lat':\n",
      "            node['pos'][0] = float(element.attrib[a])\n",
      "          else:\n",
      "            node['pos'][1] = float(element.attrib[a])\n",
      "        \n",
      "        else:\n",
      "          node[a] = element.attrib[a]\n",
      "\n",
      "      # Iterate tag children\n",
      "      for tag in element.iter(\"tag\"):\n",
      "        if not problemchars.search(tag.attrib['k']):\n",
      "            \n",
      "            # Tags with single colon and beginning with addr\n",
      "            if lower_colon.search(tag.attrib['k']) and tag.attrib['k'].find('addr') == 0:\n",
      "                if 'address' not in node:\n",
      "                    node['address'] = {}\n",
      "                \n",
      "                sub_attr = tag.attrib['k'].split(':', 1)\n",
      "                \n",
      "                if is_street_name(tag):\n",
      "                    # Do some cleaning\n",
      "                    better_name = update(tag.attrib['v'], map_street_types, street_type_updater_re)\n",
      "                    best_name = update(better_name, map_cardinal_directions, cardinal_dir_updater_re)\n",
      "                    \n",
      "                    node['address'][sub_attr[1]] = best_name\n",
      "                else:    \n",
      "                    node['address'][sub_attr[1]] = tag.attrib['v']\n",
      "\n",
      "            # All other tags that don't begin with \"addr\"\n",
      "            elif not tag.attrib['k'].find('addr') == 0:\n",
      "                if tag.attrib['k'] not in node:\n",
      "                    \n",
      "                    node[tag.attrib['k']] = tag.attrib['v']\n",
      "                else:\n",
      "                    node[\"tag:\" + tag.attrib['k']] = tag.attrib['v']\n",
      "                \n",
      "      # Iterate nd children building a list\n",
      "      for nd in element.iter(\"nd\"):\n",
      "        if 'node_refs' not in node:\n",
      "            node['node_refs'] = []\n",
      "            \n",
      "        node['node_refs'].append(nd.attrib['ref'])\n",
      "\n",
      "      return node\n",
      "    else:\n",
      "      return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now parse the XML, shape the elements, and write to a json file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "def process_map(file_in, pretty = False):\n",
      "\n",
      "    file_out = \"{0}.json\".format(file_in)\n",
      "    \n",
      "    with open(file_out, \"wb\") as fo:\n",
      "        for _, element in ET.iterparse(file_in):\n",
      "            el = shape_element(element)\n",
      "            if el:\n",
      "                if pretty:\n",
      "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
      "                else:\n",
      "                    fo.write(json.dumps(el) + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "process_map(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problems encountered in your map"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Overview of the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at the size of this file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "print \"The downloaded file is {} MB\".format(os.path.getsize(filename)/1.0e6) # convert from bytes to megabytes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The downloaded file is 71.727396 MB\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The json file is {} MB\".format(os.path.getsize(filename + \".json\")/1.0e6) # convert from bytes to megabytes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The json file is 77.050402 MB\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import signal\n",
      "import subprocess\n",
      "\n",
      "# The os.setsid() is passed in the argument preexec_fn so\n",
      "# it's run after the fork() and before  exec() to run the shell.\n",
      "pro = subprocess.Popen(\"mongod\", preexec_fn = os.setsid) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymongo import MongoClient\n",
      "\n",
      "db_name = \"osm\"\n",
      "\n",
      "client = MongoClient('localhost:27017')\n",
      "db = client[db_name]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build mongoimport command\n",
      "\n",
      "collection = filename[:filename.find(\".\")]\n",
      "working_directory = \"/Users/jasondamiani/Developer/IPython/Notebooks/\"\n",
      "json_file = filename + \".json\"\n",
      "\n",
      "mongoimport_cmd = \"mongoimport --db \" + db_name + \\\n",
      "                  \" --collection \" + collection + \\\n",
      "                  \" --file \" + working_directory + json_file\n",
      "\n",
      "# Before importing, drop collection if it exists\n",
      "if collection in db.collection_names():\n",
      "    print \"dropping collection\"\n",
      "    db[collection].drop()\n",
      "    \n",
      "subprocess.call(mongoimport_cmd.split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "number of unique users"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "volusia_flagler = db[collection]\n",
      "\n",
      "len(volusia_flagler.distinct('created.user'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "260"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "number of nodes and ways"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint.pprint(volusia_flagler.aggregate({\"$group\" : {\"_id\" : \"$type\", \"count\" : {\"$sum\" : 1}}}))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'ok': 1.0,\n",
        " u'result': [{u'_id': u'way', u'count': 28359},\n",
        "             {u'_id': u'node', u'count': 308585}]}\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "number of chosen type of nodes, like cafes, shops etc"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.killpg(pro.pid, signal.SIGTERM)  # Send the signal to all the process groups"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Other ideas about the datasets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}